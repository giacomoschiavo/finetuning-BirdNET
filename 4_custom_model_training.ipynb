{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Information Creation\n",
    "Create a json file to represent the files inside the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from birdlib import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sudo modprobe nvidia_uvm\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"dataset\"\n",
    "MODEL_NAME = 'VanillaCNN'\n",
    "DATASET_VAR = 'augm_final_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = f'../segments/{DATASET_NAME}'\n",
    "TRAIN_PATH = f\"{DATASET_PATH}/train\"\n",
    "VALID_PATH = f\"{DATASET_PATH}/valid\"\n",
    "TEST_PATH = f\"{DATASET_PATH}/test\"\n",
    "MODEL_PATH = f'./models/{MODEL_NAME}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_config(dataset_name, config_file_name='dataset_config.json'):\n",
    "    saving_path = f\"utils/{dataset_name}/{config_file_name}\"\n",
    "    if os.path.exists(saving_path):\n",
    "        print(\"Dataset config already created!\")\n",
    "        with open(saving_path) as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    mappings = utils.get_mappings(TRAIN_PATH)\n",
    "    samples = utils.collect_samples(TRAIN_PATH, VALID_PATH, TEST_PATH, mappings)\n",
    "\n",
    "    dataset_config = {\n",
    "        \"mappings\": mappings,\n",
    "        \"samples\": samples\n",
    "    }\n",
    "    with open(saving_path, \"w\") as f:\n",
    "        json.dump(dataset_config, f)\n",
    "    print(\"Saved new dataset config\")\n",
    "    return dataset_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset config already created!\n"
     ]
    }
   ],
   "source": [
    "dataset_config = create_dataset_config(DATASET_NAME, f'dataset_config_{DATASET_VAR}.json')\n",
    "mappings = dataset_config[\"mappings\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = utils.load_model_class(MODEL_NAME)\n",
    "model = model_class(len(mappings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectograms Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Aeroplane\n",
      "Processing: Muscicapa striata_Spotted Flycatcher\n",
      "Processing: Periparus ater_Coal Tit\n",
      "Processing: Regulus regulus_Goldcrest\n",
      "Processing: Anthus trivialis_Tree Pipit\n",
      "Processing: Vegetation\n",
      "Processing: Troglodytes troglodytes_Eurasian Wren\n",
      "Processing: Erithacus rubecula_European Robin\n",
      "Processing: None\n",
      "Processing: Parus major_Great Tit\n",
      "Processing: Certhia familiaris_Eurasian Treecreeper\n",
      "Processing: Phylloscopus collybita_Common Chiffchaff\n",
      "Processing: Coccothraustes coccothraustes_Hawfinch\n",
      "Processing: Wind\n",
      "Processing: Turdus merula_Eurasian Blackbird\n",
      "Processing: Loxia curvirostra_Common Crossbill\n",
      "Processing: Regulus ignicapilla_Common Firecrest\n",
      "Processing: Sylvia atricapilla_Eurasian Blackcap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Lophophanes cristatus_Crested Tit\n",
      "Processing: Fringilla coelebs_Common Chaffinch\n",
      "Processing: Aeroplane\n",
      "Processing: Muscicapa striata_Spotted Flycatcher\n",
      "Processing: Periparus ater_Coal Tit\n",
      "Processing: Regulus regulus_Goldcrest\n",
      "Processing: Anthus trivialis_Tree Pipit\n",
      "Processing: Vegetation\n",
      "Processing: Troglodytes troglodytes_Eurasian Wren\n",
      "Processing: Erithacus rubecula_European Robin\n",
      "Processing: None\n",
      "Processing: Parus major_Great Tit\n",
      "Processing: Certhia familiaris_Eurasian Treecreeper\n",
      "Processing: Phylloscopus collybita_Common Chiffchaff\n",
      "Processing: Coccothraustes coccothraustes_Hawfinch\n",
      "Processing: Wind\n",
      "Processing: Turdus merula_Eurasian Blackbird\n",
      "Processing: Loxia curvirostra_Common Crossbill\n",
      "Processing: Regulus ignicapilla_Common Firecrest\n",
      "Processing: Sylvia atricapilla_Eurasian Blackcap\n",
      "Processing: Lophophanes cristatus_Crested Tit\n",
      "Processing: Fringilla coelebs_Common Chaffinch\n",
      "Processing: Aeroplane\n",
      "Processing: Muscicapa striata_Spotted Flycatcher\n",
      "Processing: Periparus ater_Coal Tit\n",
      "Processing: Regulus regulus_Goldcrest\n",
      "Processing: Anthus trivialis_Tree Pipit\n",
      "Processing: Vegetation\n",
      "Processing: Troglodytes troglodytes_Eurasian Wren\n",
      "Processing: Erithacus rubecula_European Robin\n",
      "Processing: None\n",
      "Processing: Parus major_Great Tit\n",
      "Processing: Certhia familiaris_Eurasian Treecreeper\n",
      "Processing: Phylloscopus collybita_Common Chiffchaff\n",
      "Processing: Coccothraustes coccothraustes_Hawfinch\n",
      "Processing: Wind\n",
      "Processing: Turdus merula_Eurasian Blackbird\n",
      "Processing: Loxia curvirostra_Common Crossbill\n",
      "Processing: Regulus ignicapilla_Common Firecrest\n",
      "Processing: Sylvia atricapilla_Eurasian Blackcap\n",
      "Processing: Lophophanes cristatus_Crested Tit\n",
      "Processing: Fringilla coelebs_Common Chaffinch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'✅ Spettrogrammi generati e salvati.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPECS_TRAIN_PATH = f\"{DATASET_PATH}/train_specs\"\n",
    "SPECS_VALID_PATH = f\"{DATASET_PATH}/valid_specs\"\n",
    "SPECS_TEST_PATH = f\"{DATASET_PATH}/test_specs\"\n",
    "os.makedirs(SPECS_TRAIN_PATH, exist_ok=True)\n",
    "os.makedirs(SPECS_VALID_PATH, exist_ok=True)\n",
    "os.makedirs(SPECS_TEST_PATH, exist_ok=True)\n",
    "utils.specs_generation(TRAIN_PATH, SPECS_TRAIN_PATH, dataset_config['mappings'])\n",
    "utils.specs_generation(VALID_PATH, SPECS_VALID_PATH, dataset_config['mappings'])\n",
    "utils.specs_generation(TEST_PATH, SPECS_TEST_PATH, dataset_config['mappings'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset_config, model, model_name, dataset_var, epochs=10, batch_size=100, lr=1e-5, patience=3, early_stop_patience=10, early_stop_tollerance=1e-5, print_freq=100, load_weights=False, checkpoint_name='checkpoint.pth'):\n",
    "    history_loss = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training {model_name} on: {device}\")\n",
    "\n",
    "    model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=0.5, patience=patience\n",
    "    )\n",
    "    history_loss = []\n",
    "    history_valid_loss = []\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    saving_path = f'models/{model_name}/{dataset_var}/checkpoint.pth'\n",
    "    if load_weights:\n",
    "        if not os.path.exists(saving_path):\n",
    "            print(\"No weights found!\")\n",
    "            return None\n",
    "        checkpoint = torch.load(saving_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        history_loss = checkpoint['history_loss']\n",
    "        history_valid_loss = checkpoint['history_valid_loss']\n",
    "        best_loss = checkpoint['best_loss']\n",
    "        starting_epoch = len(history_loss)\n",
    "        print(f\"Model Loaded!\")\n",
    "        \n",
    "    print(\"Loading training data...\")\n",
    "    train_loader = utils.get_dataloader(dataset_config, split=\"train\", batch_size=batch_size)\n",
    "    valid_loader = utils.get_dataloader(dataset_config, split=\"valid\", batch_size=batch_size)\n",
    "    print(\"Loaded!\")\n",
    "    \n",
    "    model.train()\n",
    "    early_stop_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        print(f\"\\n🎯 Starting epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        for batch_index, (mel_spec, labels, _) in enumerate(train_loader):\n",
    "            mel_spec = mel_spec.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(mel_spec)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if batch_index % print_freq == 0:\n",
    "                print('Epoch: [{0}][{1}/{2}], Loss: {loss:.5f}'.format(epoch, batch_index, len(train_loader), loss=loss))\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1} - Loss: {train_loss:.5f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = 0.0\n",
    "            for mel_spec, labels, _ in valid_loader:\n",
    "                mel_spec = mel_spec.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(mel_spec)\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "            valid_loss /= len(valid_loader)\n",
    "        \n",
    "        history_loss.append(train_loss)\n",
    "        history_valid_loss.append(valid_loss)\n",
    "        scheduler.step(valid_loss)\n",
    "        \n",
    "        np.save(f'models/{model_name}/{dataset_var}/history_loss.npy', history_loss)\n",
    "        np.save(f'models/{model_name}/{dataset_var}/history_valid_loss.npy', history_valid_loss)\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            early_stop_counter = 0\n",
    "            print(f\"💾 Saving improved model at epoch {epoch+1} with train_loss={train_loss:.5f}\")\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'history_valid_loss': history_valid_loss,\n",
    "                'history_loss': history_loss,\n",
    "                'best_loss': best_loss,\n",
    "            }, saving_path)\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            print(f\"🛑 No improvement — early stop counter: {early_stop_counter}/{early_stop_patience}\")\n",
    "\n",
    "        print(f\"🔁 Epoch {epoch+1} completed - valid loss: {valid_loss:.7f} - LR: {optimizer.param_groups[0]['lr']:.1e}\")\n",
    "\n",
    "        if early_stop_counter >= early_stop_patience:\n",
    "            print(f\"\\n🚨 Early stopping triggered after {early_stop_patience} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "    torch.save(model.state_dict(), f\"models/{model_name}/{dataset_var}/final_weights.pth\")\n",
    "    print(\"✅ Training completed\")\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training VanillaCNN on: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from epoch 0 for 200 epochs\n",
      "Loading training data...\n",
      "Loaded!\n",
      "\n",
      "🎯 Starting epoch 1/200\n",
      "Epoch: [0][0/379], Loss: 0.69296\n",
      "Epoch: [0][100/379], Loss: 0.20031\n",
      "Epoch: [0][200/379], Loss: 0.21787\n",
      "Epoch: [0][300/379], Loss: 0.21195\n",
      "Epoch 1 - Loss: 0.26043\n",
      "💾 Saving improved model at epoch 1 with train_loss=0.26043\n",
      "🔁 Epoch 1 completed - valid loss: 0.2228592 - LR: 1.0e-02\n",
      "\n",
      "🎯 Starting epoch 2/200\n",
      "Epoch: [1][0/379], Loss: 0.18324\n",
      "Epoch: [1][100/379], Loss: 0.21471\n",
      "Epoch: [1][200/379], Loss: 0.20615\n",
      "Epoch: [1][300/379], Loss: 0.21426\n",
      "Epoch 2 - Loss: 0.20667\n",
      "💾 Saving improved model at epoch 2 with train_loss=0.20667\n",
      "🔁 Epoch 2 completed - valid loss: 0.2223791 - LR: 1.0e-02\n",
      "\n",
      "🎯 Starting epoch 3/200\n",
      "Epoch: [2][0/379], Loss: 0.21040\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET_VAR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_var\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATASET_VAR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 57\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(dataset_config, model, model_name, dataset_var, epochs, batch_size, lr, patience, early_stop_patience, early_stop_tollerance, print_freq, load_weights, checkpoint_name)\u001b[0m\n\u001b[1;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     55\u001b[0m epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(history_loss)\n\u001b[0;32m---> 57\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m%\u001b[39m print_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: [\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m][\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{loss:.5f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, batch_index, \u001b[38;5;28mlen\u001b[39m(train_loader), loss\u001b[38;5;241m=\u001b[39mloss))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.makedirs(f'models/{MODEL_NAME}/{DATASET_VAR}', exist_ok=True)\n",
    "model = train_model(dataset_config, model, model_name=MODEL_NAME, dataset_var=DATASET_VAR, epochs=200, batch_size=64, lr=1e-2, load_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/VanillaCNN/augm_final_test/history_loss.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history_loss \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mMODEL_NAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDATASET_VAR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/history_loss.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history_loss, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtomato\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/VanillaCNN/augm_final_test/history_loss.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "history_loss = np.load(f'models/{MODEL_NAME}/{DATASET_VAR}/history_loss.npy')\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history_loss, linestyle='-', color='tomato', label=\"Loss\")\n",
    "# plt.title(title)\n",
    "plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(ylabel)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26260"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "os.getpid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
