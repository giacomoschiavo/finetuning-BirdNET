{"Certhia familiaris_Eurasian Treecreeper": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 3}, "Dendrocopos major_Great Spotted Woodpecker": {"precision": 1.0, "recall": 1.0, "f1-score": 1.0, "support": 16}, "Dryocopus martius_Black Woodpecker": {"precision": 1.0, "recall": 0.09523809523809523, "f1-score": 0.17391304347826084, "support": 21}, "Erithacus rubecula_European Robin": {"precision": 0.9939024390243902, "recall": 0.29690346083788705, "f1-score": 0.45722300140252453, "support": 549}, "Fringilla coelebs_Common Chaffinch": {"precision": 0.8406040268456376, "recall": 0.5107033639143731, "f1-score": 0.6353836398224477, "support": 981}, "Lophophanes cristatus_Crested Tit": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 3}, "Muscicapa striata_Spotted Flycatcher": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 14}, "Periparus ater_Coal Tit": {"precision": 1.0, "recall": 0.1, "f1-score": 0.18181818181818182, "support": 20}, "Phylloscopus collybita_Common Chiffchaff": {"precision": 0.9813333333333333, "recall": 0.5500747384155455, "f1-score": 0.7049808429118775, "support": 669}, "Regulus ignicapilla_Common Firecrest": {"precision": 0.8833333333333333, "recall": 0.6883116883116883, "f1-score": 0.7737226277372263, "support": 231}, "Regulus regulus_Goldcrest": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 10}, "Sylvia atricapilla_Eurasian Blackcap": {"precision": 0.8771929824561403, "recall": 0.2053388090349076, "f1-score": 0.33277870216306155, "support": 487}, "Troglodytes troglodytes_Eurasian Wren": {"precision": 0.9375, "recall": 0.2702702702702703, "f1-score": 0.41958041958041964, "support": 111}, "Turdus merula_Eurasian Blackbird": {"precision": 0.6666666666666666, "recall": 0.2671009771986971, "f1-score": 0.3813953488372093, "support": 307}, "Turdus philomelos_Song Thrush": {"precision": 0.42424242424242425, "recall": 0.2413793103448276, "f1-score": 0.3076923076923077, "support": 58}, "micro avg": {"precision": 0.8688029020556227, "recall": 0.41293103448275864, "f1-score": 0.559797428905337, "support": 3480}, "macro avg": {"precision": 0.6403183470601284, "recall": 0.2816880475710862, "f1-score": 0.35789920769623446, "support": 3480}, "weighted avg": {"precision": 0.8759698847956403, "recall": 0.41293103448275864, "f1-score": 0.5435484503926937, "support": 3480}, "samples avg": {"precision": 0.4814680671254376, "recall": 0.42593263310394786, "f1-score": 0.4417964505613908, "support": 3480}}