{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import math\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20190601_000000.WAV',\n",
       " '20190601_010000.WAV',\n",
       " '20190601_020000.WAV',\n",
       " '20190601_030000.WAV',\n",
       " '20190601_040000.WAV']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prendi il nome di tutti i files\n",
    "audio_path = \"D:\\\\Giacomo\\\\Tovanella-20241110T120546Z-001\\\\Tovanella_repartition\"\n",
    "audio_names = glob.glob(\"D:\\\\Giacomo\\\\Tovanella-20241110T120546Z-001\\\\Tovanella_repartition\\\\*\\\\*.wav\")\n",
    "audio_names = [audio_name.split(\"\\\\\")[-1] for audio_name in audio_names]\n",
    "audio_names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_tags = scipy.io.loadmat('Bird_tags_Train.mat')[\"Bird_tags\"]\n",
    "coords = {}\n",
    "inverted_coords = {}\n",
    "for elem in bird_tags:\n",
    "    tag = elem[0][0][0][0][0]\n",
    "    file_name = elem[0][0][0][1][0]\n",
    "    bbox = elem[0][0][0][3][:4]\n",
    "    start_time = math.floor(min(bbox[:, 0]))\n",
    "    end_time = math.floor(max(bbox[:, 0]))\n",
    "    duration = end_time - start_time\n",
    "    if tag not in coords:\n",
    "        coords[tag] = []\n",
    "    if file_name not in inverted_coords:\n",
    "        inverted_coords[file_name] = []\n",
    "    coords[tag].append({\"file_name\": file_name, \"bbox\": bbox.tolist(), \"start_time\": start_time, \"duration\": duration})\n",
    "    inverted_coords[file_name].append({\"tag\": tag, \"start_time\": start_time, \"duration\": duration})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con una finestra di 3 secondi, andiamo a individuare gli uccelli\n",
    "# presenti in ciascuna finestra, e li mettiamo in un insieme\n",
    "# number_chunks = int(librosa.get_duration(y=y, sr=sr) / 3) \n",
    "number_chunks = 200\n",
    "y_true_raw = []\n",
    "for audio_name in audio_names:\n",
    "    audio_chunks = [set() for i in range(number_chunks)]\n",
    "    if audio_name not in inverted_coords.keys():    # questi file sono presenti nella cartella ma non nel .mat\n",
    "        continue\n",
    "    for bird in inverted_coords[audio_name]:\n",
    "        for i in range(bird['duration']):\n",
    "            position = (bird['start_time'] + i) // 3\n",
    "            if position < number_chunks:\n",
    "                audio_chunks[position].add(bird['tag'].replace(\"_\", \" \"))\n",
    "    y_true_raw.append(audio_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\giaco\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from birdnetlib.analyzer import Analyzer\n",
    "from birdnetlib.batch import DirectoryAnalyzer\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "all_recordings = []\n",
    "def on_analyze_complete(recording):\n",
    "    print(\"Analyzing \", recording.path)\n",
    "    # pprint(recording.detections)\n",
    "    all_recordings.append(recording)\n",
    "\n",
    "def on_error(recording, error):\n",
    "    print(\"An exception occurred: {}\".format(error))\n",
    "    print(recording.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_dir = \"D:\\\\Giacomo\\\\Tovanella-20241110T120546Z-001\\\\Tovanella\\\\\"\n",
    "dest_dir = \"D:\\\\Giacomo\\\\Tovanella-20241110T120546Z-001\\\\Tovanella_repartition\\\\\"\n",
    "# dividi i file per data in diverse cartelle\n",
    "folders = set([audio_name.split(\"_\")[0] for audio_name in audio_names])\n",
    "for folder in folders:\n",
    "    os.makedirs(dest_dir + folder, exist_ok=True)\n",
    "\n",
    "# questo sposta i file, da non usare se giÃ  usato\n",
    "# for audio_name in audio_names:\n",
    "#     folder = audio_name.split(\"_\")[0]\n",
    "#     os.rename(og_dir + audio_name, os.path.join(dest_dir, folder, audio_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Analyzer for folder  20200210\n",
      "Labels loaded.\n",
      "load model True\n",
      "Model loaded.\n",
      "Labels loaded.\n",
      "load_species_list_model\n",
      "Meta model loaded.\n",
      "Starting Watcher\n",
      "read_audio_data\n",
      "read_audio_data: complete, read  200 chunks.\n",
      "analyze_recording 20200210_000000.WAV\n",
      "recording has lon/lat\n",
      "set_predicted_species_list_from_position\n",
      "return_predicted_species_list\n",
      "6\n",
      "124 species loaded.\n",
      "Analyzing  D:\\Giacomo\\Tovanella-20241110T120546Z-001\\Tovanella_repartition\\20200210\\20200210_000000.WAV\n",
      "read_audio_data\n",
      "read_audio_data: complete, read  200 chunks.\n",
      "analyze_recording 20200210_010000.WAV\n",
      "recording has lon/lat\n",
      "set_predicted_species_list_from_position\n",
      "Analyzing  D:\\Giacomo\\Tovanella-20241110T120546Z-001\\Tovanella_repartition\\20200210\\20200210_010000.WAV\n",
      "read_audio_data\n",
      "read_audio_data: complete, read  200 chunks.\n",
      "analyze_recording 20200210_020000.WAV\n",
      "recording has lon/lat\n",
      "set_predicted_species_list_from_position\n",
      "Analyzing  D:\\Giacomo\\Tovanella-20241110T120546Z-001\\Tovanella_repartition\\20200210\\20200210_020000.WAV\n",
      "read_audio_data\n",
      "read_audio_data: complete, read  200 chunks.\n",
      "analyze_recording 20200210_030000.WAV\n",
      "recording has lon/lat\n",
      "set_predicted_species_list_from_position\n",
      "Analyzing  D:\\Giacomo\\Tovanella-20241110T120546Z-001\\Tovanella_repartition\\20200210\\20200210_030000.WAV\n",
      "read_audio_data\n",
      "read_audio_data: complete, read  200 chunks.\n",
      "analyze_recording 20200210_040000.WAV\n",
      "recording has lon/lat\n",
      "set_predicted_species_list_from_position\n",
      "Analyzing  D:\\Giacomo\\Tovanella-20241110T120546Z-001\\Tovanella_repartition\\20200210\\20200210_040000.WAV\n",
      "read_audio_data\n",
      "read_audio_data: complete, read  200 chunks.\n",
      "analyze_recording 20200210_050000.WAV\n",
      "recording has lon/lat\n",
      "set_predicted_species_list_from_position\n",
      "Analyzing  D:\\Giacomo\\Tovanella-20241110T120546Z-001\\Tovanella_repartition\\20200210\\20200210_050000.WAV\n",
      "read_audio_data\n",
      "read_audio_data: complete, read  200 chunks.\n",
      "analyze_recording 20200210_060000.WAV\n",
      "recording has lon/lat\n",
      "set_predicted_species_list_from_position\n",
      "Analyzing  D:\\Giacomo\\Tovanella-20241110T120546Z-001\\Tovanella_repartition\\20200210\\20200210_060000.WAV\n",
      "read_audio_data\n",
      "read_audio_data: complete, read  200 chunks.\n",
      "analyze_recording 20200210_070000.WAV\n",
      "recording has lon/lat\n",
      "set_predicted_species_list_from_position\n",
      "Analyzing  D:\\Giacomo\\Tovanella-20241110T120546Z-001\\Tovanella_repartition\\20200210\\20200210_070000.WAV\n",
      "read_audio_data\n",
      "read_audio_data: complete, read  200 chunks.\n",
      "analyze_recording 20200210_080000.WAV\n",
      "recording has lon/lat\n",
      "set_predicted_species_list_from_position\n",
      "Analyzing  D:\\Giacomo\\Tovanella-20241110T120546Z-001\\Tovanella_repartition\\20200210\\20200210_080000.WAV\n",
      "read_audio_data\n",
      "read_audio_data: complete, read  200 chunks.\n",
      "analyze_recording 20200210_090000.WAV\n",
      "recording has lon/lat\n",
      "set_predicted_species_list_from_position\n",
      "Analyzing  D:\\Giacomo\\Tovanella-20241110T120546Z-001\\Tovanella_repartition\\20200210\\20200210_090000.WAV\n",
      "read_audio_data\n",
      "read_audio_data: complete, read  200 chunks.\n",
      "analyze_recording 20200210_100000.WAV\n",
      "recording has lon/lat\n",
      "set_predicted_species_list_from_position\n",
      "Analyzing  D:\\Giacomo\\Tovanella-20241110T120546Z-001\\Tovanella_repartition\\20200210\\20200210_100000.WAV\n",
      "read_audio_data\n",
      "read_audio_data: complete, read  200 chunks.\n",
      "analyze_recording 20200210_110000.WAV\n",
      "recording has lon/lat\n",
      "set_predicted_species_list_from_position\n",
      "Analyzing  D:\\Giacomo\\Tovanella-20241110T120546Z-001\\Tovanella_repartition\\20200210\\20200210_110000.WAV\n",
      "read_audio_data\n",
      "read_audio_data: complete, read  200 chunks.\n",
      "analyze_recording 20200210_120000.WAV\n",
      "recording has lon/lat\n",
      "set_predicted_species_list_from_position\n",
      "Analyzing  D:\\Giacomo\\Tovanella-20241110T120546Z-001\\Tovanella_repartition\\20200210\\20200210_120000.WAV\n",
      "read_audio_data\n",
      "read_audio_data: complete, read  200 chunks.\n",
      "analyze_recording 20200210_130000.WAV\n",
      "recording has lon/lat\n",
      "set_predicted_species_list_from_position\n",
      "Analyzing  D:\\Giacomo\\Tovanella-20241110T120546Z-001\\Tovanella_repartition\\20200210\\20200210_130000.WAV\n",
      "read_audio_data\n",
      "read_audio_data: complete, read  200 chunks.\n",
      "analyze_recording 20200210_140000.WAV\n",
      "recording has lon/lat\n",
      "set_predicted_species_list_from_position\n",
      "Analyzing  D:\\Giacomo\\Tovanella-20241110T120546Z-001\\Tovanella_repartition\\20200210\\20200210_140000.WAV\n",
      "read_audio_data\n",
      "read_audio_data: complete, read  200 chunks.\n",
      "analyze_recording 20200210_150000.WAV\n",
      "recording has lon/lat\n",
      "set_predicted_species_list_from_position\n",
      "An exception occurred: \n",
      "D:\\Giacomo\\Tovanella-20241110T120546Z-001\\Tovanella_repartition\\20200210\\20200210_150000.WAV\n",
      "read_audio_data\n",
      "read_audio_data: complete, read  200 chunks.\n",
      "analyze_recording 20200210_160000.WAV\n",
      "recording has lon/lat\n",
      "set_predicted_species_list_from_position\n",
      "Analyzing  D:\\Giacomo\\Tovanella-20241110T120546Z-001\\Tovanella_repartition\\20200210\\20200210_160000.WAV\n",
      "read_audio_data\n",
      "read_audio_data: complete, read  200 chunks.\n",
      "analyze_recording 20200210_170000.WAV\n",
      "recording has lon/lat\n",
      "set_predicted_species_list_from_position\n",
      "Analyzing  D:\\Giacomo\\Tovanella-20241110T120546Z-001\\Tovanella_repartition\\20200210\\20200210_170000.WAV\n",
      "read_audio_data\n",
      "read_audio_data: complete, read  200 chunks.\n",
      "analyze_recording 20200210_180000.WAV\n",
      "recording has lon/lat\n",
      "set_predicted_species_list_from_position\n",
      "Analyzing  D:\\Giacomo\\Tovanella-20241110T120546Z-001\\Tovanella_repartition\\20200210\\20200210_180000.WAV\n",
      "read_audio_data\n"
     ]
    }
   ],
   "source": [
    "for folder in list(folders)[:2]:\n",
    "    print(\"Starting Analyzer for folder \", folder)\n",
    "    analyzer = Analyzer()\n",
    "\n",
    "    directory = os.path.join(dest_dir, folder)\n",
    "    print(\"Starting Watcher\")\n",
    "    batch = DirectoryAnalyzer(\n",
    "        directory,\n",
    "        analyzers=[analyzer],\n",
    "        lon=12.28458,\n",
    "        lat=46.31664,\n",
    "        date=datetime(year=int(folder[:4]), month=int(folder[4:6]), day=int(folder[-2:])),\n",
    "        min_conf=0.1,\n",
    "    )\n",
    "\n",
    "    batch.on_analyze_complete = on_analyze_complete\n",
    "    batch.on_error = on_error\n",
    "    batch.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_raw = []\n",
    "for recording in all_recordings:\n",
    "    model_preds = recording.detections\n",
    "    model_chunks_labels = [[] for i in range(number_chunks)]\n",
    "    recording_chunks = [set() for i in range(number_chunks)]\n",
    "    for pred in model_preds:\n",
    "        duration = int(pred['end_time'] - pred['start_time'])\n",
    "        start_time = int(pred['start_time'])\n",
    "        position = start_time // 3\n",
    "        model_chunks_labels[position].append({'tag': pred['scientific_name'], 'conf': pred['confidence']})\n",
    "        recording_chunks[position].add(pred['scientific_name'].replace(\"_\", \" \"))\n",
    "    y_pred_raw.append(recording_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Evaluation\n",
    "chunks are evaluated globally, there's no dependence to the original audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_true_labels = set()\n",
    "all_true_labels = set([key.replace(\"_\", \" \") for key in coords.keys()])\n",
    "all_pred_labels = [specie.split(\"_\")[0] for specie in analyzer.custom_species_list]\n",
    "all_pred_labels = set(all_pred_labels)\n",
    "all_labels = all_pred_labels.union(all_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([all_labels])\n",
    "\n",
    "y_true = []\n",
    "for i, record in enumerate(y_true_raw):\n",
    "    y_true.append(mlb.transform(y_true_raw[i]))\n",
    "\n",
    "y_pred = []\n",
    "for i, record in enumerate(y_pred_raw):\n",
    "    y_pred.append(mlb.transform(y_pred_raw[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009935064935064936"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "hamming_loss(y_true[3], y_pred[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8663101604278075, 0.3656884875846501, 0.5142857142857143, None)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision_recall_fscore_support(y_true[3], y_pred[3], average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[199,   1],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[199,   1],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[199,   1],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[195,   5],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[199,   1],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[136,   0],\n",
       "        [ 60,   4]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[198,   2],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[199,   1],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[ 85,   0],\n",
       "        [ 23,  92]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[199,   1],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[118,   0],\n",
       "        [ 65,  17]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[199,   1],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[ 49,   1],\n",
       "        [101,  49]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[195,   0],\n",
       "        [  5,   0]],\n",
       "\n",
       "       [[191,   9],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[179,   1],\n",
       "        [ 20,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]],\n",
       "\n",
       "       [[193,   0],\n",
       "        [  7,   0]],\n",
       "\n",
       "       [[200,   0],\n",
       "        [  0,   0]]], dtype=int64)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "multilabel_confusion_matrix(y_true[3], y_pred[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
