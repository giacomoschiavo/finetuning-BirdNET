{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Information Creation\n",
    "Create a json file to represent the files inside the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import json\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sudo modprobe nvidia_uvm\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"DATASET_CNN\"\n",
    "MODEL_NAME = 'VanillaCNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = f'../segments/{DATASET_NAME}'\n",
    "TRAIN_PATH = f\"{DATASET_PATH}/train\"\n",
    "TEST_PATH = f\"{DATASET_PATH}/test\"\n",
    "MODEL_PATH = f'./models/{MODEL_NAME}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_config(dataset_name):\n",
    "    saving_path = f\"utils/{dataset_name}/dataset_config.json\"\n",
    "    if os.path.exists(saving_path):\n",
    "        print(\"Dataset config already created!\")\n",
    "        with open(saving_path) as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    mappings = utils.get_mappings(TEST_PATH)\n",
    "    samples = utils.collect_samples(TRAIN_PATH, TEST_PATH, mappings)\n",
    "\n",
    "    dataset_config = {\n",
    "        \"mappings\": mappings,\n",
    "        \"samples\": samples\n",
    "    }\n",
    "    with open(saving_path, \"w\") as f:\n",
    "        json.dump(dataset_config, f)\n",
    "    print(\"Saved new dataset config\")\n",
    "    return dataset_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset config already created!\n"
     ]
    }
   ],
   "source": [
    "dataset_config = create_dataset_config(DATASET_NAME)\n",
    "mappings = dataset_config[\"mappings\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = utils.load_model_class(MODEL_NAME)\n",
    "model = model_class(len(mappings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectograms Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Muscicapa striata_Spotted Flycatcher\n",
      "Processing: Periparus ater_Coal Tit\n",
      "Processing: Regulus regulus_Goldcrest\n",
      "Processing: Glaucidium passerinum_Eurasian Pygmy-Owl\n",
      "Processing: Troglodytes troglodytes_Eurasian Wren\n",
      "Processing: Erithacus rubecula_European Robin\n",
      "Processing: Dryocopus martius_Black Woodpecker\n",
      "Processing: Certhia familiaris_Eurasian Treecreeper\n",
      "Processing: Pyrrhula pyrrhula_Eurasian Bullfinch\n",
      "Processing: Turdus merula_Eurasian Blackbird\n",
      "Processing: Loxia curvirostra_Common Crossbill\n",
      "Processing: Regulus ignicapilla_Common Firecrest\n",
      "Processing: Dendrocopos major_Great Spotted Woodpecker\n",
      "Processing: Sylvia atricapilla_Eurasian Blackcap\n",
      "Processing: Lophophanes cristatus_Crested Tit\n",
      "Processing: Fringilla coelebs_Common Chaffinch\n",
      "Processing: Turdus philomelos_Song Thrush\n",
      "Processing: Muscicapa striata_Spotted Flycatcher\n",
      "Processing: Periparus ater_Coal Tit\n",
      "Processing: Regulus regulus_Goldcrest\n",
      "Processing: Glaucidium passerinum_Eurasian Pygmy-Owl\n",
      "Processing: Troglodytes troglodytes_Eurasian Wren\n",
      "Processing: Erithacus rubecula_European Robin\n",
      "Processing: Dryocopus martius_Black Woodpecker\n",
      "Processing: Certhia familiaris_Eurasian Treecreeper\n",
      "Processing: Pyrrhula pyrrhula_Eurasian Bullfinch\n",
      "Processing: Turdus merula_Eurasian Blackbird\n",
      "Processing: Loxia curvirostra_Common Crossbill\n",
      "Processing: Regulus ignicapilla_Common Firecrest\n",
      "Processing: Dendrocopos major_Great Spotted Woodpecker\n",
      "Processing: Sylvia atricapilla_Eurasian Blackcap\n",
      "Processing: Lophophanes cristatus_Crested Tit\n",
      "Processing: Fringilla coelebs_Common Chaffinch\n",
      "Processing: Turdus philomelos_Song Thrush\n"
     ]
    }
   ],
   "source": [
    "SPECS_TRAIN_PATH = f\"{DATASET_PATH}/train_specs\"\n",
    "SPECS_TEST_PATH = f\"{DATASET_PATH}/test_specs\"\n",
    "os.makedirs(SPECS_TRAIN_PATH, exist_ok=True)\n",
    "os.makedirs(SPECS_TEST_PATH, exist_ok=True)\n",
    "utils.specs_generation(TRAIN_PATH, SPECS_TRAIN_PATH, dataset_config['mappings'])\n",
    "utils.specs_generation(TEST_PATH, SPECS_TEST_PATH, dataset_config['mappings'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset_config, model, epochs=10, batch_size=100, lr=1e-5, patience=3, early_stop_patience=5, print_freq=100, load_weights=False):\n",
    "    history_loss = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training {MODEL_NAME} on: {device}\")\n",
    "\n",
    "    print(\"Loading training data...\")\n",
    "    train_loader = utils.get_dataloader(dataset_config, split=\"train\", batch_size=batch_size)\n",
    "    print(\"Loaded!\")\n",
    "\n",
    "    model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=0.5, patience=patience, threshold=1e-3\n",
    "    )\n",
    "    history_loss = []\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    saving_path = f'models/{MODEL_NAME}/checkpoint.pth'\n",
    "    if load_weights:\n",
    "        if not os.path.exists(saving_path):\n",
    "            print(\"No weights found!\")\n",
    "            return None\n",
    "        checkpoint = torch.load(saving_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        history_loss = checkpoint['history_loss']\n",
    "        best_loss = checkpoint['avg_loss']\n",
    "        \n",
    "    early_stop_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        print(f\"\\nğŸ¯ Starting epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        for batch_index, (mel_spec, labels, _) in enumerate(train_loader):\n",
    "            mel_spec = mel_spec.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(mel_spec)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if batch_index % print_freq == 0:\n",
    "                print('Epoch: [{0}][{1}/{2}], Loss: {loss:.5f}'.format(epoch, batch_index, len(train_loader), loss=loss))\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        history_loss.append(running_loss)\n",
    "        scheduler.step(running_loss)\n",
    "\n",
    "        np.save(f'models/{MODEL_NAME}/history_loss.npy', history_loss)\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            early_stop_counter = 0\n",
    "            print(f\"ğŸ’¾ Saving improved model at epoch {epoch+1} with avg_loss={avg_loss:.5f}\")\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'avg_loss': avg_loss,\n",
    "                'history_loss': history_loss\n",
    "            }, saving_path)\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            print(f\"ğŸ›‘ No improvement â€” early stop counter: {early_stop_counter}/{early_stop_patience}\")\n",
    "\n",
    "        print(f\"ğŸ” Epoch {epoch+1} completed - Avg loss: {avg_loss:.5f} - LR: {optimizer.param_groups[0]['lr']:.1e}\")\n",
    "\n",
    "        if early_stop_counter >= early_stop_patience:\n",
    "            print(f\"\\nğŸš¨ Early stopping triggered after {early_stop_patience} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "    print(\"âœ… Training completed\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training VanillaCNN on: cuda\n",
      "Loading training data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded!\n",
      "\n",
      "ğŸ¯ Starting epoch 1/200\n",
      "Epoch: [0][0/254], Loss: 0.67897\n",
      "Epoch: [0][100/254], Loss: 0.23950\n",
      "Epoch: [0][200/254], Loss: 0.21625\n",
      "ğŸ’¾ Saving improved model at epoch 1 with avg_loss=0.24976\n",
      "ğŸ” Epoch 1 completed - Avg loss: 0.24976 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 2/200\n",
      "Epoch: [1][0/254], Loss: 0.22486\n",
      "Epoch: [1][100/254], Loss: 0.16672\n",
      "Epoch: [1][200/254], Loss: 0.18543\n",
      "ğŸ’¾ Saving improved model at epoch 2 with avg_loss=0.18241\n",
      "ğŸ” Epoch 2 completed - Avg loss: 0.18241 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 3/200\n",
      "Epoch: [2][0/254], Loss: 0.17406\n",
      "Epoch: [2][100/254], Loss: 0.16722\n",
      "Epoch: [2][200/254], Loss: 0.15597\n",
      "ğŸ’¾ Saving improved model at epoch 3 with avg_loss=0.15422\n",
      "ğŸ” Epoch 3 completed - Avg loss: 0.15422 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 4/200\n",
      "Epoch: [3][0/254], Loss: 0.15253\n",
      "Epoch: [3][100/254], Loss: 0.16689\n",
      "Epoch: [3][200/254], Loss: 0.14007\n",
      "ğŸ’¾ Saving improved model at epoch 4 with avg_loss=0.14454\n",
      "ğŸ” Epoch 4 completed - Avg loss: 0.14454 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 5/200\n",
      "Epoch: [4][0/254], Loss: 0.12738\n",
      "Epoch: [4][100/254], Loss: 0.15588\n",
      "Epoch: [4][200/254], Loss: 0.15182\n",
      "ğŸ’¾ Saving improved model at epoch 5 with avg_loss=0.13863\n",
      "ğŸ” Epoch 5 completed - Avg loss: 0.13863 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 6/200\n",
      "Epoch: [5][0/254], Loss: 0.14227\n",
      "Epoch: [5][100/254], Loss: 0.13171\n",
      "Epoch: [5][200/254], Loss: 0.14598\n",
      "ğŸ’¾ Saving improved model at epoch 6 with avg_loss=0.13426\n",
      "ğŸ” Epoch 6 completed - Avg loss: 0.13426 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 7/200\n",
      "Epoch: [6][0/254], Loss: 0.13350\n",
      "Epoch: [6][100/254], Loss: 0.14241\n",
      "Epoch: [6][200/254], Loss: 0.13142\n",
      "ğŸ’¾ Saving improved model at epoch 7 with avg_loss=0.13043\n",
      "ğŸ” Epoch 7 completed - Avg loss: 0.13043 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 8/200\n",
      "Epoch: [7][0/254], Loss: 0.14249\n",
      "Epoch: [7][100/254], Loss: 0.12488\n",
      "Epoch: [7][200/254], Loss: 0.13232\n",
      "ğŸ’¾ Saving improved model at epoch 8 with avg_loss=0.12629\n",
      "ğŸ” Epoch 8 completed - Avg loss: 0.12629 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 9/200\n",
      "Epoch: [8][0/254], Loss: 0.11988\n",
      "Epoch: [8][100/254], Loss: 0.13274\n",
      "Epoch: [8][200/254], Loss: 0.12521\n",
      "ğŸ’¾ Saving improved model at epoch 9 with avg_loss=0.12421\n",
      "ğŸ” Epoch 9 completed - Avg loss: 0.12421 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 10/200\n",
      "Epoch: [9][0/254], Loss: 0.10865\n",
      "Epoch: [9][100/254], Loss: 0.11813\n",
      "Epoch: [9][200/254], Loss: 0.10647\n",
      "ğŸ’¾ Saving improved model at epoch 10 with avg_loss=0.12124\n",
      "ğŸ” Epoch 10 completed - Avg loss: 0.12124 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 11/200\n",
      "Epoch: [10][0/254], Loss: 0.12476\n",
      "Epoch: [10][100/254], Loss: 0.13033\n",
      "Epoch: [10][200/254], Loss: 0.12754\n",
      "ğŸ’¾ Saving improved model at epoch 11 with avg_loss=0.11908\n",
      "ğŸ” Epoch 11 completed - Avg loss: 0.11908 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 12/200\n",
      "Epoch: [11][0/254], Loss: 0.11570\n",
      "Epoch: [11][100/254], Loss: 0.13646\n",
      "Epoch: [11][200/254], Loss: 0.11173\n",
      "ğŸ’¾ Saving improved model at epoch 12 with avg_loss=0.11756\n",
      "ğŸ” Epoch 12 completed - Avg loss: 0.11756 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 13/200\n",
      "Epoch: [12][0/254], Loss: 0.09012\n",
      "Epoch: [12][100/254], Loss: 0.12386\n",
      "Epoch: [12][200/254], Loss: 0.10457\n",
      "ğŸ’¾ Saving improved model at epoch 13 with avg_loss=0.11545\n",
      "ğŸ” Epoch 13 completed - Avg loss: 0.11545 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 14/200\n",
      "Epoch: [13][0/254], Loss: 0.12087\n",
      "Epoch: [13][100/254], Loss: 0.11546\n",
      "Epoch: [13][200/254], Loss: 0.12493\n",
      "ğŸ’¾ Saving improved model at epoch 14 with avg_loss=0.11428\n",
      "ğŸ” Epoch 14 completed - Avg loss: 0.11428 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 15/200\n",
      "Epoch: [14][0/254], Loss: 0.09527\n",
      "Epoch: [14][100/254], Loss: 0.11009\n",
      "Epoch: [14][200/254], Loss: 0.09843\n",
      "ğŸ’¾ Saving improved model at epoch 15 with avg_loss=0.11246\n",
      "ğŸ” Epoch 15 completed - Avg loss: 0.11246 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 16/200\n",
      "Epoch: [15][0/254], Loss: 0.09232\n",
      "Epoch: [15][100/254], Loss: 0.10073\n",
      "Epoch: [15][200/254], Loss: 0.12958\n",
      "ğŸ’¾ Saving improved model at epoch 16 with avg_loss=0.11126\n",
      "ğŸ” Epoch 16 completed - Avg loss: 0.11126 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 17/200\n",
      "Epoch: [16][0/254], Loss: 0.10945\n",
      "Epoch: [16][100/254], Loss: 0.09595\n",
      "Epoch: [16][200/254], Loss: 0.10727\n",
      "ğŸ’¾ Saving improved model at epoch 17 with avg_loss=0.11031\n",
      "ğŸ” Epoch 17 completed - Avg loss: 0.11031 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 18/200\n",
      "Epoch: [17][0/254], Loss: 0.10956\n",
      "Epoch: [17][100/254], Loss: 0.14571\n",
      "Epoch: [17][200/254], Loss: 0.10455\n",
      "ğŸ’¾ Saving improved model at epoch 18 with avg_loss=0.10864\n",
      "ğŸ” Epoch 18 completed - Avg loss: 0.10864 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 19/200\n",
      "Epoch: [18][0/254], Loss: 0.10333\n",
      "Epoch: [18][100/254], Loss: 0.10039\n",
      "Epoch: [18][200/254], Loss: 0.08365\n",
      "ğŸ’¾ Saving improved model at epoch 19 with avg_loss=0.10717\n",
      "ğŸ” Epoch 19 completed - Avg loss: 0.10717 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 20/200\n",
      "Epoch: [19][0/254], Loss: 0.11818\n",
      "Epoch: [19][100/254], Loss: 0.11745\n",
      "Epoch: [19][200/254], Loss: 0.10836\n",
      "ğŸ’¾ Saving improved model at epoch 20 with avg_loss=0.10617\n",
      "ğŸ” Epoch 20 completed - Avg loss: 0.10617 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 21/200\n",
      "Epoch: [20][0/254], Loss: 0.10756\n",
      "Epoch: [20][100/254], Loss: 0.11492\n",
      "Epoch: [20][200/254], Loss: 0.09943\n",
      "ğŸ’¾ Saving improved model at epoch 21 with avg_loss=0.10529\n",
      "ğŸ” Epoch 21 completed - Avg loss: 0.10529 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 22/200\n",
      "Epoch: [21][0/254], Loss: 0.11004\n",
      "Epoch: [21][100/254], Loss: 0.11814\n",
      "Epoch: [21][200/254], Loss: 0.08417\n",
      "ğŸ’¾ Saving improved model at epoch 22 with avg_loss=0.10456\n",
      "ğŸ” Epoch 22 completed - Avg loss: 0.10456 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 23/200\n",
      "Epoch: [22][0/254], Loss: 0.11099\n",
      "Epoch: [22][100/254], Loss: 0.11877\n",
      "Epoch: [22][200/254], Loss: 0.08098\n",
      "ğŸ’¾ Saving improved model at epoch 23 with avg_loss=0.10356\n",
      "ğŸ” Epoch 23 completed - Avg loss: 0.10356 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 24/200\n",
      "Epoch: [23][0/254], Loss: 0.10229\n",
      "Epoch: [23][100/254], Loss: 0.10568\n",
      "Epoch: [23][200/254], Loss: 0.08700\n",
      "ğŸ’¾ Saving improved model at epoch 24 with avg_loss=0.10303\n",
      "ğŸ” Epoch 24 completed - Avg loss: 0.10303 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 25/200\n",
      "Epoch: [24][0/254], Loss: 0.11073\n",
      "Epoch: [24][100/254], Loss: 0.09358\n",
      "Epoch: [24][200/254], Loss: 0.11232\n",
      "ğŸ’¾ Saving improved model at epoch 25 with avg_loss=0.10246\n",
      "ğŸ” Epoch 25 completed - Avg loss: 0.10246 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 26/200\n",
      "Epoch: [25][0/254], Loss: 0.10081\n",
      "Epoch: [25][100/254], Loss: 0.09764\n",
      "Epoch: [25][200/254], Loss: 0.09128\n",
      "ğŸ’¾ Saving improved model at epoch 26 with avg_loss=0.10081\n",
      "ğŸ” Epoch 26 completed - Avg loss: 0.10081 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 27/200\n",
      "Epoch: [26][0/254], Loss: 0.10126\n",
      "Epoch: [26][100/254], Loss: 0.09365\n",
      "Epoch: [26][200/254], Loss: 0.10082\n",
      "ğŸ’¾ Saving improved model at epoch 27 with avg_loss=0.10033\n",
      "ğŸ” Epoch 27 completed - Avg loss: 0.10033 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 28/200\n",
      "Epoch: [27][0/254], Loss: 0.10862\n",
      "Epoch: [27][100/254], Loss: 0.10981\n",
      "Epoch: [27][200/254], Loss: 0.10276\n",
      "ğŸ’¾ Saving improved model at epoch 28 with avg_loss=0.10011\n",
      "ğŸ” Epoch 28 completed - Avg loss: 0.10011 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 29/200\n",
      "Epoch: [28][0/254], Loss: 0.11290\n",
      "Epoch: [28][100/254], Loss: 0.09772\n",
      "Epoch: [28][200/254], Loss: 0.11239\n",
      "ğŸ’¾ Saving improved model at epoch 29 with avg_loss=0.09909\n",
      "ğŸ” Epoch 29 completed - Avg loss: 0.09909 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 30/200\n",
      "Epoch: [29][0/254], Loss: 0.09878\n",
      "Epoch: [29][100/254], Loss: 0.10379\n",
      "Epoch: [29][200/254], Loss: 0.10066\n",
      "ğŸ’¾ Saving improved model at epoch 30 with avg_loss=0.09844\n",
      "ğŸ” Epoch 30 completed - Avg loss: 0.09844 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 31/200\n",
      "Epoch: [30][0/254], Loss: 0.09055\n",
      "Epoch: [30][100/254], Loss: 0.10832\n",
      "Epoch: [30][200/254], Loss: 0.09493\n",
      "ğŸ’¾ Saving improved model at epoch 31 with avg_loss=0.09753\n",
      "ğŸ” Epoch 31 completed - Avg loss: 0.09753 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 32/200\n",
      "Epoch: [31][0/254], Loss: 0.08473\n",
      "Epoch: [31][100/254], Loss: 0.10760\n",
      "Epoch: [31][200/254], Loss: 0.08940\n",
      "ğŸ’¾ Saving improved model at epoch 32 with avg_loss=0.09737\n",
      "ğŸ” Epoch 32 completed - Avg loss: 0.09737 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 33/200\n",
      "Epoch: [32][0/254], Loss: 0.10522\n",
      "Epoch: [32][100/254], Loss: 0.07298\n",
      "Epoch: [32][200/254], Loss: 0.07261\n",
      "ğŸ’¾ Saving improved model at epoch 33 with avg_loss=0.09663\n",
      "ğŸ” Epoch 33 completed - Avg loss: 0.09663 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 34/200\n",
      "Epoch: [33][0/254], Loss: 0.08579\n",
      "Epoch: [33][100/254], Loss: 0.09915\n",
      "Epoch: [33][200/254], Loss: 0.10011\n",
      "ğŸ’¾ Saving improved model at epoch 34 with avg_loss=0.09566\n",
      "ğŸ” Epoch 34 completed - Avg loss: 0.09566 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 35/200\n",
      "Epoch: [34][0/254], Loss: 0.07232\n",
      "Epoch: [34][100/254], Loss: 0.09798\n",
      "Epoch: [34][200/254], Loss: 0.08226\n",
      "ğŸ’¾ Saving improved model at epoch 35 with avg_loss=0.09534\n",
      "ğŸ” Epoch 35 completed - Avg loss: 0.09534 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 36/200\n",
      "Epoch: [35][0/254], Loss: 0.11155\n",
      "Epoch: [35][100/254], Loss: 0.09891\n",
      "Epoch: [35][200/254], Loss: 0.08969\n",
      "ğŸ’¾ Saving improved model at epoch 36 with avg_loss=0.09479\n",
      "ğŸ” Epoch 36 completed - Avg loss: 0.09479 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 37/200\n",
      "Epoch: [36][0/254], Loss: 0.10000\n",
      "Epoch: [36][100/254], Loss: 0.09887\n",
      "Epoch: [36][200/254], Loss: 0.10033\n",
      "ğŸ’¾ Saving improved model at epoch 37 with avg_loss=0.09395\n",
      "ğŸ” Epoch 37 completed - Avg loss: 0.09395 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 38/200\n",
      "Epoch: [37][0/254], Loss: 0.07638\n",
      "Epoch: [37][100/254], Loss: 0.07759\n",
      "Epoch: [37][200/254], Loss: 0.09741\n",
      "ğŸ’¾ Saving improved model at epoch 38 with avg_loss=0.09346\n",
      "ğŸ” Epoch 38 completed - Avg loss: 0.09346 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 39/200\n",
      "Epoch: [38][0/254], Loss: 0.08568\n",
      "Epoch: [38][100/254], Loss: 0.11714\n",
      "Epoch: [38][200/254], Loss: 0.10233\n",
      "ğŸ’¾ Saving improved model at epoch 39 with avg_loss=0.09296\n",
      "ğŸ” Epoch 39 completed - Avg loss: 0.09296 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 40/200\n",
      "Epoch: [39][0/254], Loss: 0.10024\n",
      "Epoch: [39][100/254], Loss: 0.08488\n",
      "Epoch: [39][200/254], Loss: 0.10946\n",
      "ğŸ’¾ Saving improved model at epoch 40 with avg_loss=0.09218\n",
      "ğŸ” Epoch 40 completed - Avg loss: 0.09218 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 41/200\n",
      "Epoch: [40][0/254], Loss: 0.07410\n",
      "Epoch: [40][100/254], Loss: 0.08501\n",
      "Epoch: [40][200/254], Loss: 0.09418\n",
      "ğŸ’¾ Saving improved model at epoch 41 with avg_loss=0.09163\n",
      "ğŸ” Epoch 41 completed - Avg loss: 0.09163 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 42/200\n",
      "Epoch: [41][0/254], Loss: 0.07456\n",
      "Epoch: [41][100/254], Loss: 0.11899\n",
      "Epoch: [41][200/254], Loss: 0.09536\n",
      "ğŸ’¾ Saving improved model at epoch 42 with avg_loss=0.09118\n",
      "ğŸ” Epoch 42 completed - Avg loss: 0.09118 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 43/200\n",
      "Epoch: [42][0/254], Loss: 0.10215\n",
      "Epoch: [42][100/254], Loss: 0.08069\n",
      "Epoch: [42][200/254], Loss: 0.09500\n",
      "ğŸ’¾ Saving improved model at epoch 43 with avg_loss=0.09089\n",
      "ğŸ” Epoch 43 completed - Avg loss: 0.09089 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 44/200\n",
      "Epoch: [43][0/254], Loss: 0.08751\n",
      "Epoch: [43][100/254], Loss: 0.08461\n",
      "Epoch: [43][200/254], Loss: 0.09389\n",
      "ğŸ’¾ Saving improved model at epoch 44 with avg_loss=0.09001\n",
      "ğŸ” Epoch 44 completed - Avg loss: 0.09001 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 45/200\n",
      "Epoch: [44][0/254], Loss: 0.07543\n",
      "Epoch: [44][100/254], Loss: 0.06909\n",
      "Epoch: [44][200/254], Loss: 0.08231\n",
      "ğŸ’¾ Saving improved model at epoch 45 with avg_loss=0.08969\n",
      "ğŸ” Epoch 45 completed - Avg loss: 0.08969 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 46/200\n",
      "Epoch: [45][0/254], Loss: 0.08615\n",
      "Epoch: [45][100/254], Loss: 0.09461\n",
      "Epoch: [45][200/254], Loss: 0.09786\n",
      "ğŸ’¾ Saving improved model at epoch 46 with avg_loss=0.08915\n",
      "ğŸ” Epoch 46 completed - Avg loss: 0.08915 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 47/200\n",
      "Epoch: [46][0/254], Loss: 0.08123\n",
      "Epoch: [46][100/254], Loss: 0.07728\n",
      "Epoch: [46][200/254], Loss: 0.10918\n",
      "ğŸ’¾ Saving improved model at epoch 47 with avg_loss=0.08848\n",
      "ğŸ” Epoch 47 completed - Avg loss: 0.08848 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 48/200\n",
      "Epoch: [47][0/254], Loss: 0.07537\n",
      "Epoch: [47][100/254], Loss: 0.08492\n",
      "Epoch: [47][200/254], Loss: 0.10443\n",
      "ğŸ’¾ Saving improved model at epoch 48 with avg_loss=0.08789\n",
      "ğŸ” Epoch 48 completed - Avg loss: 0.08789 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 49/200\n",
      "Epoch: [48][0/254], Loss: 0.07027\n",
      "Epoch: [48][100/254], Loss: 0.08203\n",
      "Epoch: [48][200/254], Loss: 0.08342\n",
      "ğŸ’¾ Saving improved model at epoch 49 with avg_loss=0.08742\n",
      "ğŸ” Epoch 49 completed - Avg loss: 0.08742 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 50/200\n",
      "Epoch: [49][0/254], Loss: 0.08974\n",
      "Epoch: [49][100/254], Loss: 0.09568\n",
      "Epoch: [49][200/254], Loss: 0.07355\n",
      "ğŸ’¾ Saving improved model at epoch 50 with avg_loss=0.08718\n",
      "ğŸ” Epoch 50 completed - Avg loss: 0.08718 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 51/200\n",
      "Epoch: [50][0/254], Loss: 0.09078\n",
      "Epoch: [50][100/254], Loss: 0.09539\n",
      "Epoch: [50][200/254], Loss: 0.08218\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 51 completed - Avg loss: 0.08745 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 52/200\n",
      "Epoch: [51][0/254], Loss: 0.08641\n",
      "Epoch: [51][100/254], Loss: 0.09210\n",
      "Epoch: [51][200/254], Loss: 0.07444\n",
      "ğŸ’¾ Saving improved model at epoch 52 with avg_loss=0.08614\n",
      "ğŸ” Epoch 52 completed - Avg loss: 0.08614 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 53/200\n",
      "Epoch: [52][0/254], Loss: 0.07739\n",
      "Epoch: [52][100/254], Loss: 0.08843\n",
      "Epoch: [52][200/254], Loss: 0.08260\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 53 completed - Avg loss: 0.08629 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 54/200\n",
      "Epoch: [53][0/254], Loss: 0.09301\n",
      "Epoch: [53][100/254], Loss: 0.09156\n",
      "Epoch: [53][200/254], Loss: 0.08310\n",
      "ğŸ’¾ Saving improved model at epoch 54 with avg_loss=0.08563\n",
      "ğŸ” Epoch 54 completed - Avg loss: 0.08563 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 55/200\n",
      "Epoch: [54][0/254], Loss: 0.06389\n",
      "Epoch: [54][100/254], Loss: 0.08779\n",
      "Epoch: [54][200/254], Loss: 0.07473\n",
      "ğŸ’¾ Saving improved model at epoch 55 with avg_loss=0.08495\n",
      "ğŸ” Epoch 55 completed - Avg loss: 0.08495 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 56/200\n",
      "Epoch: [55][0/254], Loss: 0.08602\n",
      "Epoch: [55][100/254], Loss: 0.08571\n",
      "Epoch: [55][200/254], Loss: 0.06977\n",
      "ğŸ’¾ Saving improved model at epoch 56 with avg_loss=0.08406\n",
      "ğŸ” Epoch 56 completed - Avg loss: 0.08406 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 57/200\n",
      "Epoch: [56][0/254], Loss: 0.07682\n",
      "Epoch: [56][100/254], Loss: 0.08521\n",
      "Epoch: [56][200/254], Loss: 0.09567\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 57 completed - Avg loss: 0.08414 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 58/200\n",
      "Epoch: [57][0/254], Loss: 0.09015\n",
      "Epoch: [57][100/254], Loss: 0.09295\n",
      "Epoch: [57][200/254], Loss: 0.10462\n",
      "ğŸ’¾ Saving improved model at epoch 58 with avg_loss=0.08363\n",
      "ğŸ” Epoch 58 completed - Avg loss: 0.08363 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 59/200\n",
      "Epoch: [58][0/254], Loss: 0.06889\n",
      "Epoch: [58][100/254], Loss: 0.07364\n",
      "Epoch: [58][200/254], Loss: 0.10399\n",
      "ğŸ’¾ Saving improved model at epoch 59 with avg_loss=0.08359\n",
      "ğŸ” Epoch 59 completed - Avg loss: 0.08359 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 60/200\n",
      "Epoch: [59][0/254], Loss: 0.06801\n",
      "Epoch: [59][100/254], Loss: 0.06993\n",
      "Epoch: [59][200/254], Loss: 0.07646\n",
      "ğŸ’¾ Saving improved model at epoch 60 with avg_loss=0.08251\n",
      "ğŸ” Epoch 60 completed - Avg loss: 0.08251 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 61/200\n",
      "Epoch: [60][0/254], Loss: 0.08636\n",
      "Epoch: [60][100/254], Loss: 0.07542\n",
      "Epoch: [60][200/254], Loss: 0.07257\n",
      "ğŸ’¾ Saving improved model at epoch 61 with avg_loss=0.08233\n",
      "ğŸ” Epoch 61 completed - Avg loss: 0.08233 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 62/200\n",
      "Epoch: [61][0/254], Loss: 0.09545\n",
      "Epoch: [61][100/254], Loss: 0.08082\n",
      "Epoch: [61][200/254], Loss: 0.07407\n",
      "ğŸ’¾ Saving improved model at epoch 62 with avg_loss=0.08202\n",
      "ğŸ” Epoch 62 completed - Avg loss: 0.08202 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 63/200\n",
      "Epoch: [62][0/254], Loss: 0.06874\n",
      "Epoch: [62][100/254], Loss: 0.06720\n",
      "Epoch: [62][200/254], Loss: 0.08482\n",
      "ğŸ’¾ Saving improved model at epoch 63 with avg_loss=0.08129\n",
      "ğŸ” Epoch 63 completed - Avg loss: 0.08129 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 64/200\n",
      "Epoch: [63][0/254], Loss: 0.08938\n",
      "Epoch: [63][100/254], Loss: 0.08678\n",
      "Epoch: [63][200/254], Loss: 0.07206\n",
      "ğŸ’¾ Saving improved model at epoch 64 with avg_loss=0.08099\n",
      "ğŸ” Epoch 64 completed - Avg loss: 0.08099 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 65/200\n",
      "Epoch: [64][0/254], Loss: 0.07216\n",
      "Epoch: [64][100/254], Loss: 0.07240\n",
      "Epoch: [64][200/254], Loss: 0.08839\n",
      "ğŸ’¾ Saving improved model at epoch 65 with avg_loss=0.08079\n",
      "ğŸ” Epoch 65 completed - Avg loss: 0.08079 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 66/200\n",
      "Epoch: [65][0/254], Loss: 0.06033\n",
      "Epoch: [65][100/254], Loss: 0.09259\n",
      "Epoch: [65][200/254], Loss: 0.07811\n",
      "ğŸ’¾ Saving improved model at epoch 66 with avg_loss=0.07966\n",
      "ğŸ” Epoch 66 completed - Avg loss: 0.07966 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 67/200\n",
      "Epoch: [66][0/254], Loss: 0.10620\n",
      "Epoch: [66][100/254], Loss: 0.09306\n",
      "Epoch: [66][200/254], Loss: 0.07336\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 67 completed - Avg loss: 0.08012 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 68/200\n",
      "Epoch: [67][0/254], Loss: 0.07505\n",
      "Epoch: [67][100/254], Loss: 0.08748\n",
      "Epoch: [67][200/254], Loss: 0.09147\n",
      "ğŸ’¾ Saving improved model at epoch 68 with avg_loss=0.07948\n",
      "ğŸ” Epoch 68 completed - Avg loss: 0.07948 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 69/200\n",
      "Epoch: [68][0/254], Loss: 0.07895\n",
      "Epoch: [68][100/254], Loss: 0.06015\n",
      "Epoch: [68][200/254], Loss: 0.08285\n",
      "ğŸ’¾ Saving improved model at epoch 69 with avg_loss=0.07934\n",
      "ğŸ” Epoch 69 completed - Avg loss: 0.07934 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 70/200\n",
      "Epoch: [69][0/254], Loss: 0.07007\n",
      "Epoch: [69][100/254], Loss: 0.07690\n",
      "Epoch: [69][200/254], Loss: 0.07307\n",
      "ğŸ’¾ Saving improved model at epoch 70 with avg_loss=0.07878\n",
      "ğŸ” Epoch 70 completed - Avg loss: 0.07878 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 71/200\n",
      "Epoch: [70][0/254], Loss: 0.08538\n",
      "Epoch: [70][100/254], Loss: 0.07207\n",
      "Epoch: [70][200/254], Loss: 0.07780\n",
      "ğŸ’¾ Saving improved model at epoch 71 with avg_loss=0.07849\n",
      "ğŸ” Epoch 71 completed - Avg loss: 0.07849 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 72/200\n",
      "Epoch: [71][0/254], Loss: 0.08439\n",
      "Epoch: [71][100/254], Loss: 0.08406\n",
      "Epoch: [71][200/254], Loss: 0.08193\n",
      "ğŸ’¾ Saving improved model at epoch 72 with avg_loss=0.07757\n",
      "ğŸ” Epoch 72 completed - Avg loss: 0.07757 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 73/200\n",
      "Epoch: [72][0/254], Loss: 0.07771\n",
      "Epoch: [72][100/254], Loss: 0.08437\n",
      "Epoch: [72][200/254], Loss: 0.09175\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 73 completed - Avg loss: 0.07779 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 74/200\n",
      "Epoch: [73][0/254], Loss: 0.07140\n",
      "Epoch: [73][100/254], Loss: 0.08490\n",
      "Epoch: [73][200/254], Loss: 0.09342\n",
      "ğŸ’¾ Saving improved model at epoch 74 with avg_loss=0.07705\n",
      "ğŸ” Epoch 74 completed - Avg loss: 0.07705 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 75/200\n",
      "Epoch: [74][0/254], Loss: 0.08962\n",
      "Epoch: [74][100/254], Loss: 0.09318\n",
      "Epoch: [74][200/254], Loss: 0.06763\n",
      "ğŸ’¾ Saving improved model at epoch 75 with avg_loss=0.07667\n",
      "ğŸ” Epoch 75 completed - Avg loss: 0.07667 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 76/200\n",
      "Epoch: [75][0/254], Loss: 0.07285\n",
      "Epoch: [75][100/254], Loss: 0.07652\n",
      "Epoch: [75][200/254], Loss: 0.08042\n",
      "ğŸ’¾ Saving improved model at epoch 76 with avg_loss=0.07589\n",
      "ğŸ” Epoch 76 completed - Avg loss: 0.07589 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 77/200\n",
      "Epoch: [76][0/254], Loss: 0.08818\n",
      "Epoch: [76][100/254], Loss: 0.09911\n",
      "Epoch: [76][200/254], Loss: 0.07177\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 77 completed - Avg loss: 0.07596 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 78/200\n",
      "Epoch: [77][0/254], Loss: 0.05421\n",
      "Epoch: [77][100/254], Loss: 0.07674\n",
      "Epoch: [77][200/254], Loss: 0.07299\n",
      "ğŸ’¾ Saving improved model at epoch 78 with avg_loss=0.07547\n",
      "ğŸ” Epoch 78 completed - Avg loss: 0.07547 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 79/200\n",
      "Epoch: [78][0/254], Loss: 0.08403\n",
      "Epoch: [78][100/254], Loss: 0.08638\n",
      "Epoch: [78][200/254], Loss: 0.07613\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 79 completed - Avg loss: 0.07558 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 80/200\n",
      "Epoch: [79][0/254], Loss: 0.06968\n",
      "Epoch: [79][100/254], Loss: 0.07703\n",
      "Epoch: [79][200/254], Loss: 0.07259\n",
      "ğŸ’¾ Saving improved model at epoch 80 with avg_loss=0.07456\n",
      "ğŸ” Epoch 80 completed - Avg loss: 0.07456 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 81/200\n",
      "Epoch: [80][0/254], Loss: 0.09012\n",
      "Epoch: [80][100/254], Loss: 0.07763\n",
      "Epoch: [80][200/254], Loss: 0.06584\n",
      "ğŸ’¾ Saving improved model at epoch 81 with avg_loss=0.07437\n",
      "ğŸ” Epoch 81 completed - Avg loss: 0.07437 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 82/200\n",
      "Epoch: [81][0/254], Loss: 0.08252\n",
      "Epoch: [81][100/254], Loss: 0.07494\n",
      "Epoch: [81][200/254], Loss: 0.06979\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 82 completed - Avg loss: 0.07448 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 83/200\n",
      "Epoch: [82][0/254], Loss: 0.06229\n",
      "Epoch: [82][100/254], Loss: 0.06615\n",
      "Epoch: [82][200/254], Loss: 0.06002\n",
      "ğŸ’¾ Saving improved model at epoch 83 with avg_loss=0.07325\n",
      "ğŸ” Epoch 83 completed - Avg loss: 0.07325 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 84/200\n",
      "Epoch: [83][0/254], Loss: 0.07141\n",
      "Epoch: [83][100/254], Loss: 0.04601\n",
      "Epoch: [83][200/254], Loss: 0.05010\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 84 completed - Avg loss: 0.07338 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 85/200\n",
      "Epoch: [84][0/254], Loss: 0.07778\n",
      "Epoch: [84][100/254], Loss: 0.07934\n",
      "Epoch: [84][200/254], Loss: 0.07226\n",
      "ğŸ›‘ No improvement â€” early stop counter: 2/5\n",
      "ğŸ” Epoch 85 completed - Avg loss: 0.07328 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 86/200\n",
      "Epoch: [85][0/254], Loss: 0.06449\n",
      "Epoch: [85][100/254], Loss: 0.07456\n",
      "Epoch: [85][200/254], Loss: 0.07115\n",
      "ğŸ’¾ Saving improved model at epoch 86 with avg_loss=0.07263\n",
      "ğŸ” Epoch 86 completed - Avg loss: 0.07263 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 87/200\n",
      "Epoch: [86][0/254], Loss: 0.07019\n",
      "Epoch: [86][100/254], Loss: 0.07004\n",
      "Epoch: [86][200/254], Loss: 0.06186\n",
      "ğŸ’¾ Saving improved model at epoch 87 with avg_loss=0.07227\n",
      "ğŸ” Epoch 87 completed - Avg loss: 0.07227 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 88/200\n",
      "Epoch: [87][0/254], Loss: 0.07688\n",
      "Epoch: [87][100/254], Loss: 0.05366\n",
      "Epoch: [87][200/254], Loss: 0.08196\n",
      "ğŸ’¾ Saving improved model at epoch 88 with avg_loss=0.07161\n",
      "ğŸ” Epoch 88 completed - Avg loss: 0.07161 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 89/200\n",
      "Epoch: [88][0/254], Loss: 0.06850\n",
      "Epoch: [88][100/254], Loss: 0.07925\n",
      "Epoch: [88][200/254], Loss: 0.07705\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 89 completed - Avg loss: 0.07162 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 90/200\n",
      "Epoch: [89][0/254], Loss: 0.05025\n",
      "Epoch: [89][100/254], Loss: 0.05303\n",
      "Epoch: [89][200/254], Loss: 0.09193\n",
      "ğŸ’¾ Saving improved model at epoch 90 with avg_loss=0.07101\n",
      "ğŸ” Epoch 90 completed - Avg loss: 0.07101 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 91/200\n",
      "Epoch: [90][0/254], Loss: 0.05703\n",
      "Epoch: [90][100/254], Loss: 0.08269\n",
      "Epoch: [90][200/254], Loss: 0.04524\n",
      "ğŸ’¾ Saving improved model at epoch 91 with avg_loss=0.07060\n",
      "ğŸ” Epoch 91 completed - Avg loss: 0.07060 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 92/200\n",
      "Epoch: [91][0/254], Loss: 0.07247\n",
      "Epoch: [91][100/254], Loss: 0.07003\n",
      "Epoch: [91][200/254], Loss: 0.07107\n",
      "ğŸ’¾ Saving improved model at epoch 92 with avg_loss=0.07038\n",
      "ğŸ” Epoch 92 completed - Avg loss: 0.07038 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 93/200\n",
      "Epoch: [92][0/254], Loss: 0.08909\n",
      "Epoch: [92][100/254], Loss: 0.06929\n",
      "Epoch: [92][200/254], Loss: 0.06164\n",
      "ğŸ’¾ Saving improved model at epoch 93 with avg_loss=0.07034\n",
      "ğŸ” Epoch 93 completed - Avg loss: 0.07034 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 94/200\n",
      "Epoch: [93][0/254], Loss: 0.08975\n",
      "Epoch: [93][100/254], Loss: 0.05926\n",
      "Epoch: [93][200/254], Loss: 0.06958\n",
      "ğŸ’¾ Saving improved model at epoch 94 with avg_loss=0.06980\n",
      "ğŸ” Epoch 94 completed - Avg loss: 0.06980 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 95/200\n",
      "Epoch: [94][0/254], Loss: 0.07747\n",
      "Epoch: [94][100/254], Loss: 0.05716\n",
      "Epoch: [94][200/254], Loss: 0.08123\n",
      "ğŸ’¾ Saving improved model at epoch 95 with avg_loss=0.06960\n",
      "ğŸ” Epoch 95 completed - Avg loss: 0.06960 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 96/200\n",
      "Epoch: [95][0/254], Loss: 0.05768\n",
      "Epoch: [95][100/254], Loss: 0.06414\n",
      "Epoch: [95][200/254], Loss: 0.08631\n",
      "ğŸ’¾ Saving improved model at epoch 96 with avg_loss=0.06917\n",
      "ğŸ” Epoch 96 completed - Avg loss: 0.06917 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 97/200\n",
      "Epoch: [96][0/254], Loss: 0.06663\n",
      "Epoch: [96][100/254], Loss: 0.05584\n",
      "Epoch: [96][200/254], Loss: 0.06449\n",
      "ğŸ’¾ Saving improved model at epoch 97 with avg_loss=0.06848\n",
      "ğŸ” Epoch 97 completed - Avg loss: 0.06848 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 98/200\n",
      "Epoch: [97][0/254], Loss: 0.06102\n",
      "Epoch: [97][100/254], Loss: 0.05391\n",
      "Epoch: [97][200/254], Loss: 0.08259\n",
      "ğŸ’¾ Saving improved model at epoch 98 with avg_loss=0.06842\n",
      "ğŸ” Epoch 98 completed - Avg loss: 0.06842 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 99/200\n",
      "Epoch: [98][0/254], Loss: 0.06536\n",
      "Epoch: [98][100/254], Loss: 0.07117\n",
      "Epoch: [98][200/254], Loss: 0.06766\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 99 completed - Avg loss: 0.06881 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 100/200\n",
      "Epoch: [99][0/254], Loss: 0.06889\n",
      "Epoch: [99][100/254], Loss: 0.05872\n",
      "Epoch: [99][200/254], Loss: 0.09265\n",
      "ğŸ’¾ Saving improved model at epoch 100 with avg_loss=0.06738\n",
      "ğŸ” Epoch 100 completed - Avg loss: 0.06738 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 101/200\n",
      "Epoch: [100][0/254], Loss: 0.06334\n",
      "Epoch: [100][100/254], Loss: 0.07163\n",
      "Epoch: [100][200/254], Loss: 0.06739\n",
      "ğŸ’¾ Saving improved model at epoch 101 with avg_loss=0.06731\n",
      "ğŸ” Epoch 101 completed - Avg loss: 0.06731 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 102/200\n",
      "Epoch: [101][0/254], Loss: 0.06926\n",
      "Epoch: [101][100/254], Loss: 0.05045\n",
      "Epoch: [101][200/254], Loss: 0.06618\n",
      "ğŸ’¾ Saving improved model at epoch 102 with avg_loss=0.06690\n",
      "ğŸ” Epoch 102 completed - Avg loss: 0.06690 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 103/200\n",
      "Epoch: [102][0/254], Loss: 0.06690\n",
      "Epoch: [102][100/254], Loss: 0.04438\n",
      "Epoch: [102][200/254], Loss: 0.05359\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 103 completed - Avg loss: 0.06715 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 104/200\n",
      "Epoch: [103][0/254], Loss: 0.06228\n",
      "Epoch: [103][100/254], Loss: 0.07055\n",
      "Epoch: [103][200/254], Loss: 0.05983\n",
      "ğŸ’¾ Saving improved model at epoch 104 with avg_loss=0.06658\n",
      "ğŸ” Epoch 104 completed - Avg loss: 0.06658 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 105/200\n",
      "Epoch: [104][0/254], Loss: 0.05902\n",
      "Epoch: [104][100/254], Loss: 0.06495\n",
      "Epoch: [104][200/254], Loss: 0.07066\n",
      "ğŸ’¾ Saving improved model at epoch 105 with avg_loss=0.06573\n",
      "ğŸ” Epoch 105 completed - Avg loss: 0.06573 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 106/200\n",
      "Epoch: [105][0/254], Loss: 0.06337\n",
      "Epoch: [105][100/254], Loss: 0.05846\n",
      "Epoch: [105][200/254], Loss: 0.06323\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 106 completed - Avg loss: 0.06613 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 107/200\n",
      "Epoch: [106][0/254], Loss: 0.07450\n",
      "Epoch: [106][100/254], Loss: 0.05385\n",
      "Epoch: [106][200/254], Loss: 0.07524\n",
      "ğŸ’¾ Saving improved model at epoch 107 with avg_loss=0.06533\n",
      "ğŸ” Epoch 107 completed - Avg loss: 0.06533 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 108/200\n",
      "Epoch: [107][0/254], Loss: 0.05609\n",
      "Epoch: [107][100/254], Loss: 0.05690\n",
      "Epoch: [107][200/254], Loss: 0.06513\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 108 completed - Avg loss: 0.06548 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 109/200\n",
      "Epoch: [108][0/254], Loss: 0.06013\n",
      "Epoch: [108][100/254], Loss: 0.07620\n",
      "Epoch: [108][200/254], Loss: 0.06987\n",
      "ğŸ’¾ Saving improved model at epoch 109 with avg_loss=0.06445\n",
      "ğŸ” Epoch 109 completed - Avg loss: 0.06445 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 110/200\n",
      "Epoch: [109][0/254], Loss: 0.06310\n",
      "Epoch: [109][100/254], Loss: 0.07803\n",
      "Epoch: [109][200/254], Loss: 0.05972\n",
      "ğŸ’¾ Saving improved model at epoch 110 with avg_loss=0.06378\n",
      "ğŸ” Epoch 110 completed - Avg loss: 0.06378 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 111/200\n",
      "Epoch: [110][0/254], Loss: 0.05416\n",
      "Epoch: [110][100/254], Loss: 0.07006\n",
      "Epoch: [110][200/254], Loss: 0.06651\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 111 completed - Avg loss: 0.06392 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 112/200\n",
      "Epoch: [111][0/254], Loss: 0.05993\n",
      "Epoch: [111][100/254], Loss: 0.06445\n",
      "Epoch: [111][200/254], Loss: 0.05615\n",
      "ğŸ›‘ No improvement â€” early stop counter: 2/5\n",
      "ğŸ” Epoch 112 completed - Avg loss: 0.06416 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 113/200\n",
      "Epoch: [112][0/254], Loss: 0.04576\n",
      "Epoch: [112][100/254], Loss: 0.08450\n",
      "Epoch: [112][200/254], Loss: 0.05775\n",
      "ğŸ’¾ Saving improved model at epoch 113 with avg_loss=0.06311\n",
      "ğŸ” Epoch 113 completed - Avg loss: 0.06311 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 114/200\n",
      "Epoch: [113][0/254], Loss: 0.07425\n",
      "Epoch: [113][100/254], Loss: 0.05693\n",
      "Epoch: [113][200/254], Loss: 0.04487\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 114 completed - Avg loss: 0.06356 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 115/200\n",
      "Epoch: [114][0/254], Loss: 0.05613\n",
      "Epoch: [114][100/254], Loss: 0.05194\n",
      "Epoch: [114][200/254], Loss: 0.07523\n",
      "ğŸ’¾ Saving improved model at epoch 115 with avg_loss=0.06232\n",
      "ğŸ” Epoch 115 completed - Avg loss: 0.06232 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 116/200\n",
      "Epoch: [115][0/254], Loss: 0.05868\n",
      "Epoch: [115][100/254], Loss: 0.06194\n",
      "Epoch: [115][200/254], Loss: 0.06759\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 116 completed - Avg loss: 0.06250 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 117/200\n",
      "Epoch: [116][0/254], Loss: 0.05729\n",
      "Epoch: [116][100/254], Loss: 0.05822\n",
      "Epoch: [116][200/254], Loss: 0.07320\n",
      "ğŸ’¾ Saving improved model at epoch 117 with avg_loss=0.06201\n",
      "ğŸ” Epoch 117 completed - Avg loss: 0.06201 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 118/200\n",
      "Epoch: [117][0/254], Loss: 0.05579\n",
      "Epoch: [117][100/254], Loss: 0.04683\n",
      "Epoch: [117][200/254], Loss: 0.05394\n",
      "ğŸ’¾ Saving improved model at epoch 118 with avg_loss=0.06163\n",
      "ğŸ” Epoch 118 completed - Avg loss: 0.06163 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 119/200\n",
      "Epoch: [118][0/254], Loss: 0.07025\n",
      "Epoch: [118][100/254], Loss: 0.07496\n",
      "Epoch: [118][200/254], Loss: 0.07434\n",
      "ğŸ’¾ Saving improved model at epoch 119 with avg_loss=0.06111\n",
      "ğŸ” Epoch 119 completed - Avg loss: 0.06111 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 120/200\n",
      "Epoch: [119][0/254], Loss: 0.07439\n",
      "Epoch: [119][100/254], Loss: 0.06255\n",
      "Epoch: [119][200/254], Loss: 0.07367\n",
      "ğŸ’¾ Saving improved model at epoch 120 with avg_loss=0.06075\n",
      "ğŸ” Epoch 120 completed - Avg loss: 0.06075 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 121/200\n",
      "Epoch: [120][0/254], Loss: 0.04655\n",
      "Epoch: [120][100/254], Loss: 0.05609\n",
      "Epoch: [120][200/254], Loss: 0.07060\n",
      "ğŸ’¾ Saving improved model at epoch 121 with avg_loss=0.06018\n",
      "ğŸ” Epoch 121 completed - Avg loss: 0.06018 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 122/200\n",
      "Epoch: [121][0/254], Loss: 0.06218\n",
      "Epoch: [121][100/254], Loss: 0.07111\n",
      "Epoch: [121][200/254], Loss: 0.05974\n",
      "ğŸ’¾ Saving improved model at epoch 122 with avg_loss=0.06009\n",
      "ğŸ” Epoch 122 completed - Avg loss: 0.06009 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 123/200\n",
      "Epoch: [122][0/254], Loss: 0.05685\n",
      "Epoch: [122][100/254], Loss: 0.06276\n",
      "Epoch: [122][200/254], Loss: 0.05990\n",
      "ğŸ’¾ Saving improved model at epoch 123 with avg_loss=0.05979\n",
      "ğŸ” Epoch 123 completed - Avg loss: 0.05979 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 124/200\n",
      "Epoch: [123][0/254], Loss: 0.04656\n",
      "Epoch: [123][100/254], Loss: 0.05230\n",
      "Epoch: [123][200/254], Loss: 0.05918\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 124 completed - Avg loss: 0.05993 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 125/200\n",
      "Epoch: [124][0/254], Loss: 0.06347\n",
      "Epoch: [124][100/254], Loss: 0.06206\n",
      "Epoch: [124][200/254], Loss: 0.05647\n",
      "ğŸ’¾ Saving improved model at epoch 125 with avg_loss=0.05958\n",
      "ğŸ” Epoch 125 completed - Avg loss: 0.05958 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 126/200\n",
      "Epoch: [125][0/254], Loss: 0.06725\n",
      "Epoch: [125][100/254], Loss: 0.05749\n",
      "Epoch: [125][200/254], Loss: 0.04987\n",
      "ğŸ’¾ Saving improved model at epoch 126 with avg_loss=0.05862\n",
      "ğŸ” Epoch 126 completed - Avg loss: 0.05862 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 127/200\n",
      "Epoch: [126][0/254], Loss: 0.05723\n",
      "Epoch: [126][100/254], Loss: 0.03815\n",
      "Epoch: [126][200/254], Loss: 0.05503\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 127 completed - Avg loss: 0.05889 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 128/200\n",
      "Epoch: [127][0/254], Loss: 0.05469\n",
      "Epoch: [127][100/254], Loss: 0.05668\n",
      "Epoch: [127][200/254], Loss: 0.06464\n",
      "ğŸ’¾ Saving improved model at epoch 128 with avg_loss=0.05772\n",
      "ğŸ” Epoch 128 completed - Avg loss: 0.05772 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 129/200\n",
      "Epoch: [128][0/254], Loss: 0.05581\n",
      "Epoch: [128][100/254], Loss: 0.06527\n",
      "Epoch: [128][200/254], Loss: 0.04981\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 129 completed - Avg loss: 0.05799 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 130/200\n",
      "Epoch: [129][0/254], Loss: 0.06162\n",
      "Epoch: [129][100/254], Loss: 0.05779\n",
      "Epoch: [129][200/254], Loss: 0.06567\n",
      "ğŸ’¾ Saving improved model at epoch 130 with avg_loss=0.05747\n",
      "ğŸ” Epoch 130 completed - Avg loss: 0.05747 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 131/200\n",
      "Epoch: [130][0/254], Loss: 0.06269\n",
      "Epoch: [130][100/254], Loss: 0.05410\n",
      "Epoch: [130][200/254], Loss: 0.06493\n",
      "ğŸ’¾ Saving improved model at epoch 131 with avg_loss=0.05725\n",
      "ğŸ” Epoch 131 completed - Avg loss: 0.05725 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 132/200\n",
      "Epoch: [131][0/254], Loss: 0.04950\n",
      "Epoch: [131][100/254], Loss: 0.06365\n",
      "Epoch: [131][200/254], Loss: 0.05352\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 132 completed - Avg loss: 0.05765 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 133/200\n",
      "Epoch: [132][0/254], Loss: 0.06375\n",
      "Epoch: [132][100/254], Loss: 0.05556\n",
      "Epoch: [132][200/254], Loss: 0.06133\n",
      "ğŸ’¾ Saving improved model at epoch 133 with avg_loss=0.05683\n",
      "ğŸ” Epoch 133 completed - Avg loss: 0.05683 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 134/200\n",
      "Epoch: [133][0/254], Loss: 0.05994\n",
      "Epoch: [133][100/254], Loss: 0.05119\n",
      "Epoch: [133][200/254], Loss: 0.05654\n",
      "ğŸ’¾ Saving improved model at epoch 134 with avg_loss=0.05645\n",
      "ğŸ” Epoch 134 completed - Avg loss: 0.05645 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 135/200\n",
      "Epoch: [134][0/254], Loss: 0.05872\n",
      "Epoch: [134][100/254], Loss: 0.06239\n",
      "Epoch: [134][200/254], Loss: 0.05091\n",
      "ğŸ’¾ Saving improved model at epoch 135 with avg_loss=0.05581\n",
      "ğŸ” Epoch 135 completed - Avg loss: 0.05581 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 136/200\n",
      "Epoch: [135][0/254], Loss: 0.05535\n",
      "Epoch: [135][100/254], Loss: 0.06302\n",
      "Epoch: [135][200/254], Loss: 0.06133\n",
      "ğŸ’¾ Saving improved model at epoch 136 with avg_loss=0.05567\n",
      "ğŸ” Epoch 136 completed - Avg loss: 0.05567 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 137/200\n",
      "Epoch: [136][0/254], Loss: 0.05882\n",
      "Epoch: [136][100/254], Loss: 0.07238\n",
      "Epoch: [136][200/254], Loss: 0.05419\n",
      "ğŸ’¾ Saving improved model at epoch 137 with avg_loss=0.05524\n",
      "ğŸ” Epoch 137 completed - Avg loss: 0.05524 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 138/200\n",
      "Epoch: [137][0/254], Loss: 0.05550\n",
      "Epoch: [137][100/254], Loss: 0.05881\n",
      "Epoch: [137][200/254], Loss: 0.06328\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 138 completed - Avg loss: 0.05535 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 139/200\n",
      "Epoch: [138][0/254], Loss: 0.04660\n",
      "Epoch: [138][100/254], Loss: 0.05658\n",
      "Epoch: [138][200/254], Loss: 0.05129\n",
      "ğŸ›‘ No improvement â€” early stop counter: 2/5\n",
      "ğŸ” Epoch 139 completed - Avg loss: 0.05543 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 140/200\n",
      "Epoch: [139][0/254], Loss: 0.05075\n",
      "Epoch: [139][100/254], Loss: 0.04412\n",
      "Epoch: [139][200/254], Loss: 0.04819\n",
      "ğŸ’¾ Saving improved model at epoch 140 with avg_loss=0.05454\n",
      "ğŸ” Epoch 140 completed - Avg loss: 0.05454 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 141/200\n",
      "Epoch: [140][0/254], Loss: 0.04873\n",
      "Epoch: [140][100/254], Loss: 0.05601\n",
      "Epoch: [140][200/254], Loss: 0.04960\n",
      "ğŸ’¾ Saving improved model at epoch 141 with avg_loss=0.05396\n",
      "ğŸ” Epoch 141 completed - Avg loss: 0.05396 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 142/200\n",
      "Epoch: [141][0/254], Loss: 0.04689\n",
      "Epoch: [141][100/254], Loss: 0.07346\n",
      "Epoch: [141][200/254], Loss: 0.05145\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 142 completed - Avg loss: 0.05418 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 143/200\n",
      "Epoch: [142][0/254], Loss: 0.04647\n",
      "Epoch: [142][100/254], Loss: 0.06998\n",
      "Epoch: [142][200/254], Loss: 0.04686\n",
      "ğŸ’¾ Saving improved model at epoch 143 with avg_loss=0.05374\n",
      "ğŸ” Epoch 143 completed - Avg loss: 0.05374 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 144/200\n",
      "Epoch: [143][0/254], Loss: 0.06493\n",
      "Epoch: [143][100/254], Loss: 0.05792\n",
      "Epoch: [143][200/254], Loss: 0.06369\n",
      "ğŸ’¾ Saving improved model at epoch 144 with avg_loss=0.05353\n",
      "ğŸ” Epoch 144 completed - Avg loss: 0.05353 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 145/200\n",
      "Epoch: [144][0/254], Loss: 0.04690\n",
      "Epoch: [144][100/254], Loss: 0.05637\n",
      "Epoch: [144][200/254], Loss: 0.05632\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 145 completed - Avg loss: 0.05377 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 146/200\n",
      "Epoch: [145][0/254], Loss: 0.03723\n",
      "Epoch: [145][100/254], Loss: 0.04597\n",
      "Epoch: [145][200/254], Loss: 0.04220\n",
      "ğŸ›‘ No improvement â€” early stop counter: 2/5\n",
      "ğŸ” Epoch 146 completed - Avg loss: 0.05371 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 147/200\n",
      "Epoch: [146][0/254], Loss: 0.04864\n",
      "Epoch: [146][100/254], Loss: 0.04855\n",
      "Epoch: [146][200/254], Loss: 0.05239\n",
      "ğŸ’¾ Saving improved model at epoch 147 with avg_loss=0.05194\n",
      "ğŸ” Epoch 147 completed - Avg loss: 0.05194 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 148/200\n",
      "Epoch: [147][0/254], Loss: 0.05160\n",
      "Epoch: [147][100/254], Loss: 0.06261\n",
      "Epoch: [147][200/254], Loss: 0.05693\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 148 completed - Avg loss: 0.05235 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 149/200\n",
      "Epoch: [148][0/254], Loss: 0.05578\n",
      "Epoch: [148][100/254], Loss: 0.06659\n",
      "Epoch: [148][200/254], Loss: 0.04942\n",
      "ğŸ’¾ Saving improved model at epoch 149 with avg_loss=0.05162\n",
      "ğŸ” Epoch 149 completed - Avg loss: 0.05162 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 150/200\n",
      "Epoch: [149][0/254], Loss: 0.05113\n",
      "Epoch: [149][100/254], Loss: 0.04207\n",
      "Epoch: [149][200/254], Loss: 0.06615\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 150 completed - Avg loss: 0.05170 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 151/200\n",
      "Epoch: [150][0/254], Loss: 0.05217\n",
      "Epoch: [150][100/254], Loss: 0.05098\n",
      "Epoch: [150][200/254], Loss: 0.06236\n",
      "ğŸ’¾ Saving improved model at epoch 151 with avg_loss=0.05107\n",
      "ğŸ” Epoch 151 completed - Avg loss: 0.05107 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 152/200\n",
      "Epoch: [151][0/254], Loss: 0.03746\n",
      "Epoch: [151][100/254], Loss: 0.05517\n",
      "Epoch: [151][200/254], Loss: 0.05191\n",
      "ğŸ’¾ Saving improved model at epoch 152 with avg_loss=0.05101\n",
      "ğŸ” Epoch 152 completed - Avg loss: 0.05101 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 153/200\n",
      "Epoch: [152][0/254], Loss: 0.06284\n",
      "Epoch: [152][100/254], Loss: 0.04415\n",
      "Epoch: [152][200/254], Loss: 0.06699\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 153 completed - Avg loss: 0.05105 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 154/200\n",
      "Epoch: [153][0/254], Loss: 0.05620\n",
      "Epoch: [153][100/254], Loss: 0.04218\n",
      "Epoch: [153][200/254], Loss: 0.06059\n",
      "ğŸ’¾ Saving improved model at epoch 154 with avg_loss=0.05039\n",
      "ğŸ” Epoch 154 completed - Avg loss: 0.05039 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 155/200\n",
      "Epoch: [154][0/254], Loss: 0.05311\n",
      "Epoch: [154][100/254], Loss: 0.04283\n",
      "Epoch: [154][200/254], Loss: 0.06163\n",
      "ğŸ›‘ No improvement â€” early stop counter: 1/5\n",
      "ğŸ” Epoch 155 completed - Avg loss: 0.05043 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 156/200\n",
      "Epoch: [155][0/254], Loss: 0.04263\n",
      "Epoch: [155][100/254], Loss: 0.06548\n",
      "Epoch: [155][200/254], Loss: 0.04391\n",
      "ğŸ’¾ Saving improved model at epoch 156 with avg_loss=0.04945\n",
      "ğŸ” Epoch 156 completed - Avg loss: 0.04945 - LR: 1.0e-03\n",
      "\n",
      "ğŸ¯ Starting epoch 157/200\n",
      "Epoch: [156][0/254], Loss: 0.04705\n",
      "Epoch: [156][100/254], Loss: 0.05265\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 39\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(dataset_config, model, epochs, batch_size, lr, patience, early_stop_patience, print_freq, load_weights)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_index, (mel_spec, labels, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     38\u001b[0m     mel_spec \u001b[38;5;241m=\u001b[39m mel_spec\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 39\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     41\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(mel_spec)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_model(dataset_config, model, epochs=200, batch_size=64, lr=1e-3, load_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_loss = np.load(f'models/{MODEL_NAME}/history_loss.npy')\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history_loss, linestyle='-', color='tomato', label=\"Loss\")\n",
    "# plt.title(title)\n",
    "plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(ylabel)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
